axis.text.y=element_blank(),
axis.ticks.y=element_blank())
dem <- anes_test_set %>%
ggline(x = "dem",
y = "preds",
add = "mean_se",
color = "dem",
point.size = 1,
ggtheme = theme_minimal(),
ylab = "Feelings toward Biden",
xlab = "Democrat",
title = "Party") +
scale_color_discrete(name="Female",
breaks=c("0", "1"),
labels=c("Non-Democrat", "Democrat")) +
scale_x_discrete(labels=c("0" = "Non-Democrat",
"1" = "Democrat")) +
coord_flip() +
theme(axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank())
# side by side
xgb_plots <- (fem + dem)
xgb_plots + plot_annotation(
title = 'Feature Impacts on Feelings toward Biden',
caption = 'Predictions on test set from tuned XGBoost regression.'
)
legend(0.02, 515, legend=c("Training MSE", "Test MSE"),
col = c(amerika::amerika_palette("Republican", 1),
amerika::amerika_palette("Democrat", 1)),
lty = 1)
# load some libraries
library(tidyverse)
library(tidymodels)
library(randomForest)
library(patchwork)
library(rcfss)
library(rpart)
library(rpart.plot)
library(ranger)
set.seed(1234)
theme_set(theme_minimal())
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
m1 <- randomForest(
formula = Sale_Price ~ .,
data    = ames_train
)
m1
library(tidyverse)
library(tidymodels)
library(patchwork)
library(here)
library(tictoc)
library(ggdendro)
library(cluster)
library(factoextra)
library(skimr)
library(dbscan)
set.seed(1234)
theme_set(theme_minimal())
# k-means will give you however many clusters you search for, whether these are meaningful or not, e.g...
# create some data
x <- list(
`1` = MASS::mvrnorm(n = 300, c(-4,10), matrix(c(1.5,1,1,1.5),2)),
`2` = MASS::mvrnorm(n = 300, c(5,7), matrix(c(1,2,2,6),2)),
`3` = MASS::mvrnorm(n = 300, c(-1,1), matrix(c(4,0,0,4),2)),
`4` = MASS::mvrnorm(n = 300, c(10,-10), matrix(c(4,0,0,4),2)),
`5` = MASS::mvrnorm(n = 300, c(3,-3), matrix(c(4,0,0,4),2))
) %>%
map_df(as_tibble, .id = "cluster") %>%
rename(x = V1,
y = V2)
x.out <- x %>%
select(-cluster) %>%
# nstart: initialize centroid 20 times
mutate(k2 = kmeans(x, 2, nstart = 20)$cluster,
k3 = kmeans(x, 3, nstart = 20)$cluster,
k4 = kmeans(x, 4, nstart = 20)$cluster,
k5 = kmeans(x, 5, nstart = 20)$cluster,
k6 = kmeans(x, 6, nstart = 20)$cluster,
k7 = kmeans(x, 7, nstart = 20)$cluster)
# plot clusters
x.out %>%
gather(K, pred, k2:k7) %>%
mutate(K = parse_number(K),
pred = factor(pred)) %>%
ggplot(aes(x, y, color = pred)) +
facet_wrap(~ K, labeller = label_both) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2") +
theme(legend.position = "none")
# plot clusters
x.out %>%
gather(K, pred, k2:k7) %>%
mutate(K = parse_number(K),
pred = factor(pred)) %>%
ggplot(aes(x, y, color = pred)) +
facet_wrap(~ K, labeller = label_both) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2") +
theme(legend.position = "none")
# First with k = 5
# rerun: tidyverse's simulate--from purr
kmean.out <- rerun(6, kmeans(x %>%
select(-cluster),
5,
nstart = 1))
withinss <- rep(map_chr(kmean.out, ~ .$tot.withinss), each = nrow(x))
kmean.out %>%
map_df(~ enframe(.$cluster, name = NULL), .id = "id") %>%
bind_cols(bind_rows(x, x, x, x, x, x)) %>%
mutate(withinss = str_c("Within SS = ", withinss),
id = str_c("Attempt #", id),
value = factor(value)) %>%
ggplot(aes(x, y, color = value)) +
facet_wrap(~ id + withinss, ncol = 3,
labeller = label_wrap_gen(multi_line = TRUE)) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2", guide = FALSE) +
labs(title = "Convergence of k-means cluster algorithm",
x = expression(X[1]),
y = expression(X[2])) +
theme(legend.position = "none")
# now, with 3
kmean.out <- rerun(6, kmeans(x %>%
select(-cluster),
3,
nstart = 1))
withinss <- rep(map_chr(kmean.out, ~ .$tot.withinss), each = nrow(x))
kmean.out %>%
map_df(~ enframe(.$cluster, name = NULL), .id = "id") %>%
bind_cols(bind_rows(x, x, x, x, x, x)) %>%
mutate(withinss = str_c("Within SS = ", withinss),
id = str_c("Attempt #", id),
value = factor(value)) %>%
ggplot(aes(x, y, color = value)) +
facet_wrap(~ id + withinss, ncol = 3, labeller = label_wrap_gen(multi_line = TRUE)) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2", guide = FALSE) +
labs(title = "Convergence of k-means cluster algorithm",
x = expression(X[1]),
y = expression(X[2])) +
theme(legend.position = "none")
# Another look at k-means with real data
df <- USArrests %>%
na.omit %>%
scale()
head(df)
# inspect reduction in within-cluster variation (sums of squares) over a range of values for k
wss <- function(k) {
kmeans(df, k, nstart = 10)$tot.withinss
}
tibble(
k = 1:10
) %>%
mutate(wss = map_dbl(k, wss)) %>%
ggplot(aes(k, wss)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:10) +
labs(title = "USArrests",
x = "Number of clusters K",
y = "Total within-clusters sum of squares")
# do it all with a helper function to compute average silhouette value for k clusters
avg_sil <- function(k) {
km.res <- kmeans(df, centers = k, nstart = 25)
ss <- silhouette(km.res$cluster, dist(df))
mean(ss[, 3])
}
tibble(
k = 2:10
) %>%
mutate(avg_sil = map_dbl(k, avg_sil)) %>%
ggplot(aes(k, avg_sil)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 2:10) +
labs(title = "US Arrests Data",
x = "Number of clusters K",
y = "Average silhouette value")
# simulated data
# generate data
x <- tibble(x1 = rnorm(50) + 3,
x2 = rnorm(50) - 4,
y = ifelse(x1 < 3, "1",
ifelse(x2 > -4, "2", "3")))
ggplot(x, aes(x1, x2, color = y)) +
geom_point() +
scale_color_brewer(type = "qual",
palette = "Dark2") +
labs(title = "Simulated data",
x = expression(X[1]),
y = expression(X[2])) +
theme(legend.position = "none")
# compare linkage methods
complete <- hclust(dist(x),
method = "complete")
single <- hclust(dist(x),
method = "single")
average <- hclust(dist(x),
method = "average")
# plot
ggdendrogram(complete) +
labs(title = "Complete linkage")
ggdendrogram(single) +
labs(title = "Single linkage")
ggdendrogram(average) +
labs(title = "Average linkage")
#
# A little deeper exploration focusing on HAC with complete linkage for tree cutting
# fit HAC with complete linkage
hc_complete <- hclust(dist(x),
method = "complete")
# plot
ggdendrogram(hc_complete)
# First, try cutting the tree at a height of 4 splits the dendrogram into two subtrees, and therefore two clusters:
h <- 4
# extract dendro data
hcdata <- dendro_data(hc_complete)
hclabs <- label(hcdata) %>%
left_join(tibble(label = as.factor(seq.int(nrow(x))),
cl = as.factor(cutree(hc_complete, h = h))))
# plot dendrogram
ggdendrogram(hc_complete, labels = FALSE) +
geom_text(data = hclabs,
aes(label = label, x = x, y = 0, color = cl),
vjust = .5, angle = 90) +
geom_hline(yintercept = h, linetype = 2) +
scale_color_brewer(type = "qual", palette = "Dark2", guide = FALSE) +
theme(axis.text.x = element_blank(),
legend.position = "none")
# now try for k = 3
h <- 3
# extract dendro data
hcdata <- dendro_data(hc_complete)
hclabs <- label(hcdata) %>%
left_join(tibble(label = as.factor(seq.int(nrow(x))),
cl = as.factor(cutree(hc_complete, h = h))))
# plot dendrogram
ggdendrogram(hc_complete, labels = FALSE) +
geom_text(data = hclabs,
aes(label = label, x = x, y = 0, color = cl),
vjust = .5, angle = 90) +
geom_hline(yintercept = h, linetype = 2) +
scale_color_brewer(type = "qual", palette = "Dark2", guide = FALSE) +
theme(axis.text.x = element_blank(),
legend.position = "none")
# load libraries needed for this section
library(tidyverse)
library(mixtools)
library(plotGMM)
library(amerika)
pres <- read_csv("2012_DVS.csv") %>%
mutate(State = X1,
DVS = dem_vs) %>%
select(-c(X1, dem_vs))
head(pres)
pres <- read_csv("..\Data\2012_DVS.csv") %>%
mutate(State = X1,
DVS = dem_vs) %>%
select(-c(X1, dem_vs))
pres <- read_csv("..\\Data\\2012_DVS.csv") %>%
mutate(State = X1,
DVS = dem_vs) %>%
select(-c(X1, dem_vs))
head(pres)
# Take a look at the density
ggplot(pres, aes(x = DVS)) +
geom_density() +
xlim(min(pres$DVS) - 10,
max(pres$DVS) + 10) +
theme_minimal() +
labs(x = "Democratic Vote Share")
# load libraries needed for this section
library(tidyverse)
library(mixtools)
library(plotGMM)
library(amerika)
pres <- read_csv("..\\Data\\2012_DVS.csv") %>%
mutate(State = X1,
DVS = dem_vs) %>%
select(-c(X1, dem_vs))
head(pres)
# Take a look at the density
ggplot(pres, aes(x = DVS)) +
geom_density() +
xlim(min(pres$DVS) - 10,
max(pres$DVS) + 10) +
theme_minimal() +
labs(x = "Democratic Vote Share")
# best guess at component means (fig from lecture)
ggplot(pres, aes(x = DVS)) +
geom_density() +
xlim(min(pres$DVS) - 10,
max(pres$DVS) + 10) +
theme_minimal() +
labs(x = "Democratic Vote Share") +
geom_vline(xintercept = 41,
col = amerika_palettes$Republican[3]) +
geom_vline(xintercept = 53,
col = amerika_palettes$Democrat[3])
# Start by fitting a two component (cluster) gmm
set.seed(7355)
gmm1 <- normalmixEM(pres$DVS, k = 2)
ggplot(data.frame(x = gmm1$x)) +
geom_histogram(aes(x, ..density..), alpha = 0.4, fill = "darkgray") +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm1$mu[1], gmm1$sigma[1], lam = gmm1$lambda[1]),
colour = amerika_palettes$Republican[3]) +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm1$mu[2], gmm1$sigma[2], lam = gmm1$lambda[2]),
colour = amerika_palettes$Democrat[3]) +
xlab("Democratic Vote Shares") +
ylab("Density") +
theme_minimal()
# next attempt
set.seed(7355)
gmm2 <- normalmixEM(pres$DVS, k = 3)
ggplot(data.frame(x = gmm2$x)) +
geom_histogram(aes(x, ..density..), alpha = 0.4, fill = "darkgray") +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm2$mu[1], gmm2$sigma[1], lam = gmm2$lambda[1]),
colour = amerika_palettes$Republican[3]) +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm2$mu[2], gmm2$sigma[2], lam = gmm2$lambda[2]),
colour = amerika_palettes$Democrat[3]) +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm2$mu[3], gmm2$sigma[3], lam = gmm2$lambda[3]),
colour = "black") +
xlab("Democratic Vote Shares") +
ylab("Density") +
theme_minimal()
# Searching for (potentially) problematic observation, given poor fit of GMM
which(pres$DVS > 80)
# now we can try again without outlier
pres2 <- pres[-c(17), ]
# quickly compare to make sure it worked
withDC <- head(pres$DVS, 20)
withoutDC <- head(pres2$DVS, 20)
head(data.frame(cbind(withDC, withoutDC)), 20)
# now on with the GMM
set.seed(1)
dvs.nodc <- pres2$DVS
gmm.nodc <- normalmixEM(dvs.nodc, k = 2)
ggplot(data.frame(x = gmm.nodc$x)) +
geom_histogram(aes(x, ..density..), alpha = 0.4, fill = "darkgray", bins = 20) +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm.nodc$mu[1], gmm.nodc$sigma[1], lam = gmm.nodc$lambda[1]),
colour = amerika_palettes$Republican[3]) +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm.nodc$mu[2], gmm.nodc$sigma[2], lam = gmm.nodc$lambda[2]),
colour = amerika_palettes$Democrat[3]) +
xlab("Democratic Vote Shares") +
ylab("Density") +
theme_minimal()
# Call specific values from the output
# means
gmm.nodc$mu
# sd's
gmm.nodc$sigma
# weights
gmm.nodc$lambda
## Explore component densities a bit
# Table for viz
posterior <- data.frame(cbind(gmm.nodc$x, gmm.nodc$posterior))
rownames(posterior) <- pres2$State
round(head(posterior, 10), 3)
# get counts for each component
posterior$component <- ifelse(posterior$comp.1 > 0.3, 1, 2)
table(posterior$component)
# View the DVS by component (again, saying nothing of parties explicitly)
ggplot(posterior, aes(x = V1)) +
geom_histogram(aes(fill = factor(component)), alpha = 0.4, stat ="bin", binwidth = 3) +
labs(x = "Democratic Vote Share",
y = "Count of States",
title = "Gaussian Mixture Model") +
scale_fill_manual(values=c(amerika_palettes$Republican[3], amerika_palettes$Democrat[3]),
name="Component",
breaks=c("1", "2"),
labels=c("1", "2")) +
geom_vline(xintercept = 50, linetype="solid",
color = "black", size=1.2) +
theme_minimal()
# Load data and update the pres vote data for 2012
library(tidyverse)
pres <- read_csv("..\\Data\\2012_DVS.csv") %>%
mutate(State = X1,
DVS = dem_vs) %>%
select(-c(X1, dem_vs))
View(pres)
# estimate k clusters
pres.out <- pres %>%
select(-cluster) %>%
# nstart: initialize centroid 20 times
mutate(k2 = kmeans(x, 2, nstart = 20)$cluster\)
# estimate k clusters
pres.out <- pres %>%
select(-cluster) %>%
# nstart: initialize centroid 20 times
mutate(k2 = kmeans(x, 2, nstart = 20)$cluster)
# estimate k clusters
pres.out <- pres %>%
# nstart: initialize centroid 20 times
mutate(k2 = kmeans(x, 2, nstart = 20)$cluster)
# estimate k clusters
pres.out <- pres %>%
# nstart: initialize centroid 20 times
mutate(k2 = kmeans(pres, 2, nstart = 20)$cluster)
pres <- read_csv("..\\Data\\2012_DVS.csv") %>%
mutate(State = X1,
DVS = dem_vs) %>%
select(-c(X1, dem_vs))
# estimate k clusters
pres.out <- pres %>%
# nstart: initialize centroid 20 times
mutate(k2 = kmeans(pres, 2, nstart = 20)$cluster)
# estimate k clusters
pres.out <- pres %>%
# nstart: initialize centroid 20 times
mutate(k2 = kmeans(pres$DVS, 2, nstart = 20)$cluster)
View(pres.out)
# plot clusters
pres.out %>%
gather(K, pred, k2) %>%
mutate(K = parse_number(K),
pred = factor(pred)) %>%
ggplot(aes(x, y, color = pred)) +
facet_wrap(~ K, labeller = label_both) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2") +
theme(legend.position = "none")
View(pres.out)
# plot clusters
pres.out %>%
gather(K, pred, k2) %>%
mutate(K = parse_number(K),
pred = factor(pred)) %>%
ggplot(aes(pres.out$State, pres.out$DVS, color = pred)) +
facet_wrap(~ K, labeller = label_both) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2") +
theme(legend.position = "none")
# plot clusters
pres.out %>%
gather(K, pred, k2) %>%
mutate(K = parse_number(K),
pred = factor(pred)) %>%
ggplot(aes(pres.out$DVS, pres.out$State, color = pred)) +
facet_wrap(~ K, labeller = label_both) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2") +
theme(legend.position = "none")
# 3. Put a vertical cut point/line at 50. Given that in a normal election, it requires 50+1 shares of the votes to win, the idea here is if the k-means solution is good, then all states clustered together should lie on their respective sides of the line.
pres.out %>%
gather(K, pred, k2) %>%
mutate(K = parse_number(K),
pred = factor(pred)) %>%
ggplot(aes(pres.out$DVS, pres.out$State, color = pred)) +
facet_wrap(~ K, labeller = label_both) +
geom_point() +
scale_color_brewer(type = "qual", palette = "Dark2") +
theme(legend.position = "none")+
geom_vline(xintercept = 50, linetype="solid",
color = "black", size=1.2)
# best guess at component means (fig from lecture)
ggplot(pres, aes(x = DVS)) +
geom_density() +
xlim(min(pres$DVS) - 10,
max(pres$DVS) + 10) +
theme_minimal() +
labs(x = "Democratic Vote Share") +
geom_vline(xintercept = 41,
col = amerika_palettes$Republican[3]) +
geom_vline(xintercept = 53,
col = amerika_palettes$Democrat[3])
ggplot(data.frame(x = gmm1$x)) +
geom_histogram(aes(x, ..density..), alpha = 0.4, fill = "darkgray") +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm1$mu[1], gmm1$sigma[1], lam = gmm1$lambda[1]),
colour = amerika_palettes$Republican[3]) +
stat_function(geom = "line", fun = plot_mix_comps,
args = list(gmm1$mu[2], gmm1$sigma[2], lam = gmm1$lambda[2]),
colour = amerika_palettes$Democrat[3]) +
xlab("Democratic Vote Shares") +
ylab("Density") +
theme_minimal()
pres_k<-pres %>% mutate(.,cluster=presso$cluster)
ggplot(pres_k,aes(x = DVS)) +
geom_histogram(aes(fill=factor(cluster)), alpha = 0.4,stat="bin",bindwidth=3) +
xlab("Democratic Vote Shares") +
ylab("Count of States") +
theme_minimal()
library(ggplot2)
pres_k<-pres %>% mutate(.,cluster=presso$cluster)
presso <- kmeans(pres$DVS,2, nstart = 20)
pres_k<-pres %>% mutate(.,cluster=presso$cluster)
ggplot(pres_k,aes(x = DVS)) +
geom_histogram(aes(fill=factor(cluster)), alpha = 0.4,stat="bin",bindwidth=3) +
xlab("Democratic Vote Shares") +
ylab("Count of States") +
theme_minimal()
# or histogram
ggplot(pres_k,aes(x = DVS)) +
geom_histogram(aes(fill=factor(cluster)), alpha = 0.4,stat="bin",bindwidth=3) +
xlab("Democratic Vote Shares") +
ylab("Count of States") +
geom_vline(xintercept = 50,
col = amerika_palettes$Republican[3]) +
theme_minimal()
