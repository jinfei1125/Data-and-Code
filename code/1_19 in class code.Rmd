---
title: "Model Fitting, Bias, & Variance"
author: "Philip Waggoner, MACS 30100 <br /> University of Chicago"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

library(tidyverse)
library(here)

set.seed(1234)
theme_set(theme_minimal()) 
```

# Statistical learning

> Attempt to summarize relationships between variables by reducing the dimensionality of the data

# Improve Shamwow sales

```{r echo = TRUE, eval = FALSE}
# get advertising data
advertising <- read_csv(here("data", "Advertising.csv")) %>%
  tbl_df() %>%
  select(-X1)
```

```{r echo = TRUE, eval = FALSE}
# plot separate facets for relationship between ad spending and sales
plot_ad <- advertising %>%
  gather(method, spend, -Sales) %>%
  ggplot(aes(spend, Sales)) +
  facet_wrap(~ method, scales = "free_x") +
  geom_point() +
  labs(x = "Spending (in thousands of dollars)"); plot_ad
```

# Parametric methods

```{r echo = TRUE, eval = FALSE}
method_model <- function(df) {
  lm(Sales ~ spend, data = df)
}

ad_pred <- advertising %>%
  gather(method, spend, -Sales) %>%
  group_by(method) %>%
  nest() %>%
  mutate(model = map(data, method_model),
         pred = map(model, broom::augment)) %>%
  unnest(pred)

plot_ad +
  geom_smooth(method = "lm", se = FALSE) +
  geom_linerange(data = ad_pred,
                 aes(ymin = Sales, ymax = .fitted),
                 color = "blue",
                 alpha = .5) 
```

# Non-parameteric: Locally weighted scatterplot smoothing

```{r echo = TRUE, eval = FALSE}
library(broom) # model operations
library(lattice)  # for the data

mod <- loess(NOx ~ E, 
             data = ethanol, 
             degree = 1, 
             span = .75)

fit <- augment(mod)

ggplot(fit, aes(E, NOx)) +
  geom_point() +
  geom_line(aes(y = .fitted), color = "red") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides")
```

# Optimism of training error

Take a look at the random set of data points from lecture. 

```{r echo = TRUE, eval = FALSE}
# simulate data from ISL figure 2.9
sim_mse <- tibble(
  x = runif(n = 50, min = 0, max = 100),
  y = 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3 + rnorm(50, sd = 0.6)
)

# model fit
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  stat_function(fun = function(x) 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3) +
  geom_smooth(aes(color = "lm"), method = lm, se = FALSE) +
  geom_smooth(aes(color = "spline-low"), method = lm,
              formula = y ~ splines::ns(x, 5), se = FALSE) +
  geom_smooth(aes(color = "spline-high"), method = lm,
              formula = y ~ splines::ns(x, 20), se = FALSE) +
  scale_color_brewer(type = "qual") +
  labs(title = "Training data points",
       subtitle = "Models estimated on training set",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

As we said in the lecture, the main idea is that as the flexibility increases, the models are more likely to fit the data; the training MSE will decrease. But this does not guarantee the test MSE also decreases.

Here, I applied the models fit on the training set to a new test data set generated from the same underlying process. That is, everything is identical, even the DGP. Just this time, we are changing the set of data, and overlaying the original fit lines to the new data.

```{r echo = TRUE, eval = FALSE}
sim_mse_test <- tibble(
  x = runif(n = 50, min = 0, max = 100),
  y = 5.055901 - 0.1848551 * x + 0.00748706 * x^2 - 0.00005543478 * x^3 + rnorm(50, sd = 0.6)
)

# model fit
ggplot(sim_mse, aes(x, y)) +
  geom_point(data = sim_mse_test) +
  stat_function(fun = function(x) 5.055901 - 0.1848551*x + 0.00748706*x^2 - 0.00005543478*x^3) +
  geom_smooth(aes(color = "lm"), method = lm, se = FALSE) +
  geom_smooth(aes(color = "spline-low"), method = lm,
              formula = y ~ splines::ns(x, 5), se = FALSE) +
  geom_smooth(aes(color = "spline-high"), method = lm,
              formula = y ~ splines::ns(x, 20), se = FALSE) +
  scale_color_brewer(type = "qual") +
  labs(title = "Test data points",
       subtitle = "Models estimated on training set",
       x = expression(X),
       y = expression(Y)) +
  theme(legend.position = "none")
```

It's not as clear as you might hope, but still we can see that these models no longer perform as well as compared to the training data, especially the *overfit purple line*. 

The point? When we rely only on training error, we essentially fool ourselves into thinking we've found the best model, because as we continue to tune to the training data, and run into the threat of overfitting to the training data. Thus we get too optimistic. 

That is, the model starts to detect artifacts and random patterns specific to the single set of observations but which may not really exist in the DGP. 

# Bias-variance trade-off

This brings us to the BV tradeoff. 

If we could get rid of all bias and variance such that, `MSE = irreducible error`, then we would be left with something called the *oracle* predictor because it is *impossible* to actually achieve. 

So, short of finding the oracle predictor, we need to try to build a model that finds the sweet spot between bias and variance. 

Recall a few things from the lecture:

Bias: Error that is introduced by approximating a real-life problem using a simplified model

    $$\text{Bias} = \E[\hat{f}(x_o)] - f(x_0)$$

Variance: amount by which $\hat{f}$ would change if we estimated it using a different training data set

    $$\text{Variance} = \E[\hat{f}(x_0) - \E[\hat{f}(x_0)]]^2$$

That is, the variance is the expected squared deviation of our function around its mean. 

So, in plain English, the bias is the ability of the learner to get predictions right, and the variance is the ability to do so many times with new data (i.e., “consistency”)

Let's revisit some of the figures from lecture to see these principles in action. First, plot the data we previously simulated, based on the figure 2.9 from ISL:

```{r echo = TRUE, eval = FALSE}
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  labs(title = "Training data",
       x = expression(X),
       y = expression(Y))
```

Recall, that an example of a low bias, high variance model is an extremely local version of kNN, which here `k = 1`. 

```{r echo = TRUE, eval = FALSE}
# estimate
sim_knn1 <- FNN::knn.reg(train = sim_mse,
                         test = sim_mse,
                         y = sim_mse$y,
                         k = 1)

# plot
sim_mse %>%
  mutate(pred = sim_knn1$pred) %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_step(aes(y = pred)) +
  labs(title = "Training data",
       x = expression(X),
       y = expression(Y))
```

And on the opposite extreme, a high bias, low variance model is simply the mean:

```{r echo = TRUE, eval = FALSE}
ggplot(sim_mse, aes(x, y)) +
  geom_point() +
  geom_hline(yintercept = mean(sim_mse$y)) +
  labs(title = "Training data",
       x = expression(X),
       y = expression(Y))
```

Overall, all of these are far too extreme to be useful, so there must be a middle ground. Well, what exactly do we want? More or less variance? More or less bias? Let's revisit the dart-board from lecture:

```{r echo = TRUE, eval = FALSE}
# set number of throws
n_games <- 100

# throw `n_games` of darts and get the coordinates where they hit
dart_game <- function(n_games, 
                       accurate = TRUE, 
                       consistent = TRUE) {
  if (accurate & consistent) {
    xvals <- rnorm(n_games, mean = 0, sd = .05)
    yvals <- rnorm(n_games, mean = 0, sd = .05)
  } else if (accurate == TRUE & consistent == FALSE) {
    xvals <- rnorm(n_games, mean = .5, sd = .05)
    yvals <- rnorm(n_games, mean = .4, sd = .05)
  } else if (accurate == FALSE & consistent == TRUE) {
    xvals <- rnorm(n_games, mean = 0, sd = .3)
    yvals <- rnorm(n_games, mean = 0, sd = .3)
  } else if (accurate == FALSE & consistent == FALSE) {
    xvals <- rnorm(n_games, mean = .5, sd = .3)
    yvals <- rnorm(n_games, mean = -.4, sd = .3)
  }
  
  tibble(
    x = xvals,
    y = yvals,
    accurate = accurate,
    consistent = consistent
  )
}

# get data for each situation
throws <- bind_rows(
  dart_game(n_games, accurate = TRUE, consistent = TRUE),
  dart_game(n_games, accurate = TRUE, consistent = FALSE),
  dart_game(n_games, accurate = FALSE, consistent = TRUE),
  dart_game(n_games, accurate = FALSE, consistent = FALSE)
) %>%
  mutate(
    accurate = ifelse(accurate, "Low Variance", "High Variance"),
    consistent = ifelse(consistent, "Low Bias", "High Bias")
  )

# plot the dart board, facet by each type
ggplot(data = throws, aes(x, y)) +
  facet_grid(accurate ~ consistent) +
  ggforce::geom_circle(aes(x = NULL, y = NULL, x0 = 0, y0 = 0, r = 1)) +
  geom_point(alpha = 0.5) +
  xlim(-1, 1) +
  ylim(-1, 1) +
  coord_fixed() +
  labs(title = NULL,
       x = NULL,
       y = NULL) +
  annotate("point", x = 0, y = 0, size = 3)
```

\newpage

# On your own

For this section, you will work in small groups of 4-5. *I will create these groups at random*. 

**IMPORTANT**: _Don't forget that this code you're working on here is due at the appropriate Canvas module (in the form of an attachment to a "Discussion" post) prior to 5:00 pm CDT today. You need only submit a **single** file/script to be considered for credit (i.e., this .Rmd with your code inserted below each question). Recall, I don't care whether you got things right. I only care that attempts to each question have been made._ 

We will now walk through some of the techniques covered this week and last, but this time using real data. Specifically, for this set of exercises, you will use the 2016 American National Election Pilot Study (ANES). Load the data:

```{r echo = TRUE, eval = FALSE}
library(tidyverse)
library(here)

anes <- read_csv(here("data", "anes_2016.csv"))
```

With the data loaded, answer the following questions. The objective here is twofold: 1) to practice your statistical computing skills, and 2) apply and explore error from fitting models on different sets of data.

---
title: "1.19code"
author: "Jinfei Zhu"
date: "2021/1/19"
output: pdf_document
---
# On your own

For this section, you will work in small groups of 4-5. *I will create these groups at random*. 

**IMPORTANT**: _Don't forget that this code you're working on here is due at the appropriate Canvas module (in the form of an attachment to a "Discussion" post) prior to 5:00 pm CDT today. You need only submit a **single** file/script to be considered for credit (i.e., this .Rmd with your code inserted below each question). Recall, I don't care whether you got things right. I only care that attempts to each question have been made._ 

We will now walk through some of the techniques covered this week and last, but this time using real data. Specifically, for this set of exercises, you will use the 2016 American National Election Pilot Study (ANES). Load the data:

```{r}
library(tidyverse)
library(here)
# setwd("D:\\UChicago-2021-Winter\\perspectives-on-computational-modeling\\github-code\\Data-and-Code")
# anes <- read.csv("anes_2016.csv")
anes <- read.csv(here("data", "anes_2016.csv"))
```
anes <- read.csv(here("data", "anes_2016.csv"))

With the data loaded, answer the following questions. The objective here is twofold: 1) to practice your statistical computing skills, and 2) apply and explore error from fitting models on different sets of data.

1. Using some of the techniques we covered last week:

    a. Select only the Obama feeling thermometer (`ftobama`), household income (`faminc`), party affiliation on a 3 point scale (`pid3`), birth year (`birthyr`), and gender (`gender`) (*be sure to recode missing values to `NA` and omit these*)
  
    b. Split the subset data into training (75%) and testing (25%) sets (*hint*: remember to set the seed (`set.seed()`) prior to creating the split, as the proportions are generated at random)
  
    c. Plot the distributions of each against each other to ensure they look similar
```{r echo = TRUE, eval = TRUE}
library(ggplot2)
library(dplyr)
library(tidymodels)
# 1a
anes_clean <- anes %>%
  select(ftobama,faminc,pid3,birthyr,gender) %>%
  na.omit()
anes_clean

# 1b
set.seed(1234)
split_anes <- initial_split(anes_clean,
                            prop = 0.75)
train_anes <- training(split_anes)
test_anes  <- testing(split_anes)

par(mfrow=c(2,3))
# 1c
 hist(anes_clean$ftobama) 
 hist(anes_clean$faminc) 
 hist(anes_clean$pid3) 
 hist(anes_clean$birthyr) 
 hist(anes_clean$gender) 


```


2. Fit a linear regression (`lm()`) on the *training* data, predicting obama approval ('ftobama') as a function of all other features.
```{r echo = TRUE, eval = TRUE}
# create
train_anes
model <- lm(ftobama ~ faminc + pid3 + birthyr + gender, data = train_anes)
summary(model)
```


3. Calculate the training mean squared error (*hint*: consider using the `mse()` function from Dr. Soltoff's `rcfss` package, which is at the uc-cfss github, *not* on CRAN).
```{r}
# install.packages(('devtools'))
library(devtools)
# install_github('uc-cfss/rcfss')
# ?mse
library(rcfss)
obama_hat <- predict(model, train_anes)
train_mse <- rcfss::mse(data=train_anes, ftobama, obama_hat)
train_mse

```

4. Calculate predictions for the testing set, using the model you built on the training set (*hint*: consider either `predict()` from base R, or `augment()` from `broom`).
```{r}
obama_test <- predict(model, test_anes, interval="confidence")
```


5. Calculate the testing mean squared error.
```{r}
obama_hat_test <- predict(model, test_anes)
test_mse <- rcfss::mse(test_anes, ftobama, obama_hat_test)
test_mse
```

6. Compare the mean squared error from both sets numerically, side-by-side. What do you see? *Discuss in your groups and record a few sentences as a response.*
```{r}
test_mse$.estimate
train_mse$.estimate
```


7. Write your own function to calculate the MSE. Then, use it to re-answer questions 3 and 5. Present the results here, and compare with the `rcfss` approach via `mse()`. These results should be identical to the `mse()` version. Are they? If not, *why* do you think? (*just a sentence or two on your general thoughts if they differ*) 
```{r}

mse <- function(true, estimate){
  1/length(true)*(sum((true - estimate)^2))
}

mse(test_anes$ftobama, predict(model,test_anes))
mse(train_anes$ftobama, predict(model,train_anes))
```
Good news! They are exactly the same!
